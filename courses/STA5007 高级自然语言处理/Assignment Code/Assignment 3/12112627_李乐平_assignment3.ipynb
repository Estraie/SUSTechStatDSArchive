{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045dba58",
   "metadata": {},
   "source": [
    "# Advanced NLP Assignment 3\n",
    "\n",
    "12112627 李乐平\n",
    "\n",
    "## Q1\n",
    "\n",
    "**(1)**\n",
    " Explain three different points between P-Tuning V1 and P-Tuning V2.\n",
    " \n",
    "\n",
    "- **Applied tasks**: P-Tuning V1 can apply to Knowledge Probing and NLU tasks, while P-Tuning V2 adapts to Sequence Tagging and NLU tasks.\n",
    "- **Classification head**: P-Tuning V1 uses a language modeling head to predict verbalizers, which are natural language tokens that correspond to labels. P-Tuning V2 uses a linear classifier on top of the [CLS] token or the target tokens, which is simpler and more compatible with sequence labeling tasks.\n",
    "- **Reparameterization**: P-Tuning V1 always uses a reparameterization encoder such as an MLP to transform the trainable embeddings. P-Tuning V2 finds that the usefulness of reparameterization depends on the tasks and datasets, and sometimes a simple embedding layer is enough.\n",
    "\n",
    "**(2)**\n",
    " Write code to finetune Tiny-BERT with P-Tuning v2 method using the MRPC Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471fb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --user --upgrade peft datasets\n",
    "# !python -m pip install --upgrade numpy\n",
    "# !python -m pip install --user transformers\n",
    "# !python -m pip install transformers \n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4165afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d71c3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./Q1_model_checkpoint_tinybert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23000' max='23000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23000/23000 09:58, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.615650</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.600451</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.594400</td>\n",
       "      <td>0.591131</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>0.585770</td>\n",
       "      <td>0.691176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.657700</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.589000</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.723039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.552800</td>\n",
       "      <td>0.559171</td>\n",
       "      <td>0.710784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.553132</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.596000</td>\n",
       "      <td>0.549417</td>\n",
       "      <td>0.718137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.574100</td>\n",
       "      <td>0.546513</td>\n",
       "      <td>0.725490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.558300</td>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.710784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.540062</td>\n",
       "      <td>0.718137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.551800</td>\n",
       "      <td>0.532916</td>\n",
       "      <td>0.727941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.531001</td>\n",
       "      <td>0.723039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.534366</td>\n",
       "      <td>0.730392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.521835</td>\n",
       "      <td>0.718137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.526082</td>\n",
       "      <td>0.730392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.522787</td>\n",
       "      <td>0.732843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>0.526338</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.532187</td>\n",
       "      <td>0.740196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.519039</td>\n",
       "      <td>0.727941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.508480</td>\n",
       "      <td>0.723039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.507789</td>\n",
       "      <td>0.727941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.519288</td>\n",
       "      <td>0.727941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.515907</td>\n",
       "      <td>0.732843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>0.512596</td>\n",
       "      <td>0.732843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.529400</td>\n",
       "      <td>0.508484</td>\n",
       "      <td>0.727941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>0.505932</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.598600</td>\n",
       "      <td>0.497445</td>\n",
       "      <td>0.740196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.503402</td>\n",
       "      <td>0.737745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.511460</td>\n",
       "      <td>0.742647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.496613</td>\n",
       "      <td>0.742647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.491063</td>\n",
       "      <td>0.745098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.503439</td>\n",
       "      <td>0.747549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.508357</td>\n",
       "      <td>0.742647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.487564</td>\n",
       "      <td>0.745098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.495335</td>\n",
       "      <td>0.742647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.487529</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.492452</td>\n",
       "      <td>0.752451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>0.484340</td>\n",
       "      <td>0.762255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>0.485553</td>\n",
       "      <td>0.752451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.483834</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.489478</td>\n",
       "      <td>0.752451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.486107</td>\n",
       "      <td>0.754902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.574700</td>\n",
       "      <td>0.485467</td>\n",
       "      <td>0.759804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.497947</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.494636</td>\n",
       "      <td>0.759804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>0.477031</td>\n",
       "      <td>0.757353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.577200</td>\n",
       "      <td>0.481729</td>\n",
       "      <td>0.762255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.479748</td>\n",
       "      <td>0.754902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.467456</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.461070</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.458720</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.468278</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.471589</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.551600</td>\n",
       "      <td>0.461250</td>\n",
       "      <td>0.767157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.457032</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.466518</td>\n",
       "      <td>0.776961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.459065</td>\n",
       "      <td>0.767157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.466955</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.453998</td>\n",
       "      <td>0.767157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.460701</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>0.462919</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.520800</td>\n",
       "      <td>0.454992</td>\n",
       "      <td>0.762255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>0.457343</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.454347</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>0.449792</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.518200</td>\n",
       "      <td>0.444926</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.444348</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.455310</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.451216</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.624200</td>\n",
       "      <td>0.452157</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.450320</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.444709</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.489300</td>\n",
       "      <td>0.446481</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.445873</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>0.451090</td>\n",
       "      <td>0.769608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.449341</td>\n",
       "      <td>0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.561900</td>\n",
       "      <td>0.442284</td>\n",
       "      <td>0.776961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.442873</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.449420</td>\n",
       "      <td>0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.442921</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>0.448166</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>0.442990</td>\n",
       "      <td>0.776961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.443780</td>\n",
       "      <td>0.774510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.445645</td>\n",
       "      <td>0.776961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.448386</td>\n",
       "      <td>0.784314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.442891</td>\n",
       "      <td>0.784314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.443472</td>\n",
       "      <td>0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.443937</td>\n",
       "      <td>0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.443317</td>\n",
       "      <td>0.781863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.488900</td>\n",
       "      <td>0.445842</td>\n",
       "      <td>0.784314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.442357</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.442563</td>\n",
       "      <td>0.776961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.454400</td>\n",
       "      <td>0.441283</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.441215</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.530300</td>\n",
       "      <td>0.441079</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.441259</td>\n",
       "      <td>0.776961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType, PeftType, PromptEncoderConfig\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Load the accuracy metric\n",
    "global_metric = load_metric(\"./accuracy/accuracy.py\")\n",
    "\n",
    "# Load the MRPC dataset\n",
    "dataset = load_dataset(\n",
    "    path = \"json\", \n",
    "    data_dir = \"./Q1_MRPC_dataset\", \n",
    "    data_files = {\"train\": \"train.jsonl\", \"test\": \"test.jsonl\", \"validation\": \"validation.jsonl\"}\n",
    ")\n",
    "\n",
    "# Load the Tiny-BERT model\n",
    "model_dir = \"./Q1_model_checkpoint_tinybert/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n",
    "\n",
    "# # Apply P-Tuning V2 to the model, where \"prefix_projection = True\" indicates P-Tuning V2 is used\n",
    "# peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_CLS, num_virtual_tokens=5, prefix_projection=True)\n",
    "# model = get_peft_model(model, peft_config) \n",
    "\n",
    "# Apply P-Tuning V1 to the model\n",
    "peft_config = PromptEncoderConfig(task_type=TaskType.SEQ_CLS, num_virtual_tokens=5)\n",
    "model = get_peft_model(model, peft_config) \n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text1\"], example[\"text2\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the optimizer and the learning rate scheduler\n",
    "# # For P-tuning V2\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataset[\"train\"]) * 100)\n",
    "\n",
    "# For P-tuning V1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataset[\"train\"]) * 100)\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-4, # For P-Tuning V2\n",
    "    learning_rate=3e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "#     weight_decay=1e-4, # For P-Tuning V2\n",
    "    weight_decay=2e-5,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Define the metric computing function\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = global_metric\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define the training function\n",
    "def m_train():\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=None,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "# print(dataset[\"train\"])\n",
    "    \n",
    "# Train the model\n",
    "m_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040acff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.49014005064964294,\n",
       " 'eval_accuracy': 0.7576811594202899,\n",
       " 'eval_runtime': 0.7411,\n",
       " 'eval_samples_per_second': 2327.537,\n",
       " 'eval_steps_per_second': 36.431}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the evaluation function\n",
    "def m_evaluate():\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        data_collator=None,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    result = trainer.evaluate()\n",
    "    return result\n",
    "\n",
    "m_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2a876",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Efficient finetuning with adapter method.\n",
    "\n",
    "**1**\n",
    "Explain the adapter-based finetuning. You can also refer to the Adapter Hub blog.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "Adapter-based fine-tuning is a methodology used in natural language processing (NLP) where pre-trained language models, such as BERT or GPT, are augmented with task-specific adapters rather than retraining the entire model from scratch. Adapters are small, task-specific neural network modules that are inserted into the pre-trained model's architecture, connecting to its hidden layers. During fine-tuning, only the parameters within the adapters and a minimal set of surrounding parameters are updated, leaving the majority of the pre-trained model untouched. This approach allows for efficient adaptation to new tasks with significantly fewer training resources compared to full model retraining, enabling faster deployment and reducing the risk of overfitting to the specific task at hand. Adapter-based fine-tuning strikes a balance between leveraging the generalization capabilities of pre-trained models and adapting to task-specific requirements.\n",
    "\n",
    "\n",
    "**2**\n",
    "Suppose the dimension of the input to the adapter is $ d_m $. How to change the dimension of the hidden state before the non-linear function to $ \\frac {8d_m}{3} $ ?\n",
    "\n",
    "**Answer**:\n",
    "According to the source code, we can find out that the dimensional change is done in the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31 class Adapter(nn.Module):\n",
    "# 32     \"\"\"\n",
    "# 33     Implementation of a sequential bottleneck adapter block.\n",
    "# 34     \"\"\"\n",
    "# 35 \n",
    "# 36     def __init__(\n",
    "# 37         self,\n",
    "# 38         adapter_name,\n",
    "# 39         input_size,\n",
    "# 40         down_sample,\n",
    "# 41         config: BnConfig,\n",
    "# 42     ):\n",
    "\n",
    "# ...\n",
    "\n",
    "# 73         if config[\"phm_layer\"]:\n",
    "# 74             # Linear down projection of the input\n",
    "# 75             seq_list.append(PHMLayer(adapter_name, self.input_size, self.down_sample, \"down\", config))\n",
    "# 76         else:\n",
    "# 77             seq_list.append(nn.Linear(self.input_size, self.down_sample))\n",
    "# 78 \n",
    "# 79         # select non-linearity\n",
    "# 80         self.non_linearity = Activation_Function_Class(config[\"non_linearity\"].lower())\n",
    "# 81 \n",
    "# 82         seq_list.append(self.non_linearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b6ce8",
   "metadata": {},
   "source": [
    "Note that the projection is done in line 75/77, so what we need to do is set down_sample = 8 * dm // 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f19b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter = Adapter(\"MyAdapter\", my_input_size, 8 * my_input_size // 3)  # Sample usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aadb0f",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Select an LLM like ChatGPT, 讯飞星火, etc. Design prompts to write story for 5-year-old child. The topic, style, length and clues can be provided as constraints.\n",
    "\n",
    "**1.**\n",
    "Write your prompt and generated story.\n",
    "\n",
    "**Prompt**:\n",
    "\n",
    "Topic: Adventure in a Magical Forest\n",
    "\n",
    "Style: Playful and Imaginative\n",
    "\n",
    "Length: 300-500 words\n",
    "\n",
    "Clues: A talking squirrel, a flying rainbow butterfly, a friendly dragon, a hidden treasure\n",
    "\n",
    "**Generated Story**:\n",
    "\n",
    "Once upon a time, in a land far, far away, there was a magical forest known as Enchanted Eldwood. This whimsical woodland was home to creatures of extraordinary charm and wonder. Our adventure begins with a curious explorer named Lily, who, armed with a heart full of curiosity and a backpack full of snacks, stepped into the lush greenery of Enchanted Eldwood.\n",
    "\n",
    "As Lily ventured deeper into the forest, the trees whispered secrets, and the leaves giggled with delight. Suddenly, a chittering sound caught her attention. To her surprise, a talking squirrel named Nutters appeared, perched on a tree branch.\n",
    "\n",
    "\"Hello, dear adventurer! Welcome to Enchanted Eldwood,\" Nutters greeted with a mischievous twinkle in his eye. \"I've been waiting for someone like you to share a grand adventure!\"\n",
    "\n",
    "With Nutters as her guide, Lily continued her journey, the foliage parting to reveal a breathtaking sight—a flying rainbow butterfly gracefully dancing in the air. Its wings shimmered with every color of the spectrum, leaving a trail of magic in its wake.\n",
    "\n",
    "\"Meet Iris, the Rainbow Flutterfly!\" Nutters exclaimed. \"Legend has it that following Iris leads to the heart of the forest, where a friendly dragon guards a hidden treasure.\"\n",
    "\n",
    "Eagerly, Lily followed Iris through the enchanted grove until they reached a cozy clearing. There, nestled among the wildflowers, was a dragon named Ember, its scales reflecting the warm hues of a crackling bonfire. To Lily's surprise, Ember greeted her with a gentle smile.\n",
    "\n",
    "\"Welcome, kind adventurer! Nutters told me you would come seeking the treasure,\" Ember rumbled in a voice as soothing as the rustling leaves.\n",
    "\n",
    "Ember shared the tale of the hidden treasure—a magical chest containing dreams, laughter, and joy. To unlock it, Lily needed to solve a riddle that echoed through the ancient trees.\n",
    "\n",
    "As Lily pondered the riddle, Nutters gathered acorns, and Iris fluttered around in anticipation. With a gleam in her eye, Lily solved the riddle, and the magical chest opened, releasing a burst of shimmering light that filled the entire forest.\n",
    "\n",
    "The once-hidden treasure transformed Enchanted Eldwood into an even more enchanting haven, where laughter echoed through the trees and dreams floated on the breeze.\n",
    "\n",
    "With gratitude in her heart, Lily bid farewell to her newfound friends—Nutters, Iris, and Ember. As she stepped out of the magical forest, she carried with her the joy and wonder of Enchanted Eldwood, forever grateful for the playful adventure that had unfolded among the talking squirrel, flying rainbow butterfly, friendly dragon, and the hidden treasure that sparked a world of magic.\n",
    "\n",
    "**2.**\n",
    "Discuss approaches to improve the diversity and quality of generated story.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "First, prompts should encourage specificity, prompting the model with detailed settings, character backgrounds, or unique scenarios to steer the narrative towards more intricate and imaginative paths. Additionally, incorporating emotional cues and thematic elements in prompts can help infuse stories with depth and resonance. To address diversity, prompts should encompass a broad spectrum of genres, cultures, and perspectives, fostering inclusivity and avoiding unintentional biases. Experimenting with open-ended prompts that allow for creative interpretation can also stimulate the model to produce narratives that transcend conventional story arcs, resulting in a richer and more varied storytelling output.\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
