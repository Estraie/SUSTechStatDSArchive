{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd3bcb5",
   "metadata": {},
   "source": [
    "# Advanced NLP Assignment 2\n",
    "12112627 李乐平"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81450fac",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Write the pytorch code for the bind network, the input is image_feature and output is\n",
    "transformered_image_feature . Define the \\_\\_init__() and forward() function. You\n",
    "can add anything/arguments you like in the code. Note that the bind network has three\n",
    "blocks ( )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962862c6",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fc626b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Shape: torch.Size([1, 64])\n",
      "Image Feature: tensor([[ 1.3874, -1.5726, -1.3388, -1.0086,  0.7970,  2.5626,  0.2656, -0.2886,\n",
      "          1.9697,  1.8278, -0.6941, -0.3214, -2.7398,  0.2542,  0.3267,  0.5992,\n",
      "         -1.6390, -0.5445, -1.8141, -0.4989,  0.0957, -0.1325, -1.3822, -1.2853,\n",
      "          1.6776,  1.7085, -0.7028,  1.0429,  0.9523, -0.3974,  1.1003, -0.0918,\n",
      "          0.2438, -0.3119, -0.0368,  1.0498, -1.4760,  0.9999,  0.4980, -1.9578,\n",
      "          0.6779,  0.2079,  0.4291, -0.4033,  2.9337, -0.7550,  1.9450, -0.8512,\n",
      "          1.2409,  0.7904,  0.3042, -1.3916, -0.5970, -0.8465, -1.0813, -0.9439,\n",
      "          0.9729,  0.3848,  0.5232, -0.0292,  0.5456, -1.1885, -0.6250, -0.3410]])\n",
      "Output Shape: torch.Size([1, 512])\n",
      "Output: tensor([[-0.6790, -0.4598, -0.3136,  2.2281,  0.2798,  0.7672,  0.5229, -0.1448,\n",
      "          0.1670,  1.0647, -0.3899, -0.6772, -1.4385,  0.2184,  0.7808, -1.1050,\n",
      "          0.7648,  0.7238, -0.8145,  0.6065,  1.2087,  1.4226,  0.1107, -1.1490,\n",
      "          0.6066,  1.6156,  0.2304, -2.9906,  0.6499, -0.3870, -0.7582,  1.2478,\n",
      "          1.0797,  0.6800,  2.4183,  1.3473,  0.2718,  1.4041,  0.5853, -0.2894,\n",
      "          1.2984, -0.3243, -0.7392, -0.3412,  0.6381,  1.0599, -0.6536,  1.3608,\n",
      "         -0.8283,  0.5268,  1.3062, -0.3730,  0.1469,  0.8579,  0.6458, -1.2272,\n",
      "         -0.7672,  0.6986, -0.9615, -0.4958,  1.5251, -0.8008,  1.0973, -0.6409,\n",
      "          0.4277, -0.3172, -1.4169, -1.2859,  0.0303,  0.9142, -0.2676, -0.5513,\n",
      "         -0.8714, -1.7472,  0.0106, -0.2253, -0.4375,  0.3067,  1.2455, -0.5412,\n",
      "         -0.1921,  0.4615,  0.7368,  0.5248,  0.3915, -1.5148, -2.1236, -1.5438,\n",
      "         -0.6802,  0.2211, -0.8736,  0.2697,  1.9449, -0.0986, -0.9458, -0.6939,\n",
      "         -1.5034,  1.2888, -0.1756, -0.9771,  0.7184,  0.8390,  0.8781,  0.3058,\n",
      "          0.4129,  1.0496,  0.1917,  1.7881, -0.6505, -1.4418, -0.1427, -0.2707,\n",
      "         -0.2101, -1.1419, -0.5678,  1.4635,  1.9440, -2.4392,  0.7701, -1.5685,\n",
      "          0.9151,  0.0469, -0.3881,  0.7405,  0.1746,  0.2728,  1.5190,  1.3654,\n",
      "          0.0721, -0.1819,  0.2905, -0.9221,  0.7404,  1.4318, -0.4776,  0.1716,\n",
      "          0.2780, -1.8927, -1.5187,  0.3882, -0.1316, -0.3065,  0.3094,  1.3717,\n",
      "         -0.2743, -1.1617,  0.4496,  0.1977, -0.3845, -1.5280,  0.5331,  0.8965,\n",
      "          0.9047,  0.1112, -0.5626,  0.1225,  0.3262,  0.0056, -0.1839,  0.6858,\n",
      "          0.4681, -1.2842,  1.5957, -0.0232, -0.0716, -0.9080, -0.5511,  1.0291,\n",
      "          0.5471,  1.0003, -0.1937,  1.4583, -2.0380,  1.0373,  0.4650, -0.9650,\n",
      "         -0.6406,  0.3510, -1.5876, -1.0314,  0.0268, -0.1694, -0.6740,  0.4005,\n",
      "          0.5360,  1.7395, -0.0259, -0.3516,  0.8906, -0.6932, -0.2622, -0.8253,\n",
      "         -1.4262,  0.2614,  0.5440, -0.7130, -1.0459,  0.8378, -0.9284, -0.2783,\n",
      "          0.2697, -0.9417, -0.7775, -0.5701, -1.4288,  0.9666, -0.0962,  1.1636,\n",
      "         -0.8115, -1.9502, -0.1707,  1.3440, -0.0519,  0.6770,  0.3680,  2.4515,\n",
      "         -0.8137, -1.4449,  0.1050,  0.5151, -1.3559,  0.6362, -0.4766, -1.9157,\n",
      "          0.5024,  0.9776, -1.6773,  1.2226,  1.8901, -0.7098, -1.6319, -1.6017,\n",
      "          0.0215, -0.8018, -1.1596,  1.3446, -1.3563, -0.9978, -0.1072, -0.1630,\n",
      "          0.4005, -0.5818,  0.1628,  0.8895, -1.5076,  0.5825, -1.2931, -0.2375,\n",
      "         -0.8038,  1.3873, -0.1414,  0.6222, -0.4711, -1.0411, -0.5086,  1.4348,\n",
      "          1.2858, -0.1962,  0.0254,  1.6552, -0.3153, -2.6008, -0.5229,  0.2509,\n",
      "         -1.6434, -0.5401,  1.2799, -0.6224, -1.0829,  1.0296,  1.3077, -1.8123,\n",
      "         -0.0827, -0.6490, -0.2317, -2.0023,  1.4324, -1.5903,  1.1783, -0.0347,\n",
      "          0.1353, -0.2419, -0.2086,  0.0263, -0.4535,  0.8588, -1.3399,  0.0661,\n",
      "          0.9287, -0.1546,  0.8708,  2.1357, -1.3614, -1.5350,  0.2318, -0.2103,\n",
      "         -0.5996, -0.9510,  0.9081, -0.7470,  1.2649,  0.9661, -0.7535, -0.1509,\n",
      "          0.1435, -1.0488, -0.7666,  2.1481,  0.4426, -0.4903, -1.8289, -0.8295,\n",
      "          0.1376,  0.2898, -1.4153, -0.5553, -0.6261, -0.3533, -0.3920,  0.7876,\n",
      "          0.2658, -0.1090, -0.8394, -0.1306,  1.0901, -1.9824, -1.7382,  0.6802,\n",
      "         -0.4186, -0.6996,  0.6051,  0.2613, -0.0036, -2.2257, -0.8364,  0.2020,\n",
      "          0.1735,  1.1098,  0.1610,  1.2194, -0.9842,  0.0585,  0.3462, -1.1608,\n",
      "         -1.4340, -1.6996, -0.5305, -0.6115, -0.3805, -1.9193,  1.9304,  1.5394,\n",
      "         -0.0732,  0.8849, -0.1687, -1.1015, -0.2659, -0.3338, -1.2602, -0.5942,\n",
      "          1.0026, -0.7472,  0.0729, -0.9359, -0.8892, -1.6718,  0.9245,  1.0869,\n",
      "          1.5193,  0.5137,  1.2991,  0.2285,  0.7881, -0.6102, -0.3872, -0.5026,\n",
      "         -0.5572,  0.7795, -0.3651,  0.9060,  1.0152,  1.2727,  1.4663,  1.4605,\n",
      "          0.1383, -2.3016,  0.5239,  1.0849,  0.3085, -1.0676, -0.3208,  0.7574,\n",
      "          1.1057, -0.5755, -0.8983,  0.4125, -0.2380, -1.6523, -0.0100,  0.7683,\n",
      "         -2.3471,  0.2964,  0.7271,  0.0524,  0.4914,  1.2638, -0.1677,  1.3520,\n",
      "         -1.8832,  0.9809,  0.6110,  1.5393,  0.8047, -2.5254,  0.7917,  1.4463,\n",
      "         -0.5037,  2.0833,  0.5778,  1.3531, -0.3533,  0.3636,  1.2234, -0.3079,\n",
      "          0.5875,  0.7588,  0.4572,  0.0168, -0.1770, -0.1306, -2.1348, -0.4426,\n",
      "          0.0060, -0.3337,  1.3581,  0.8708,  0.9754, -0.5025, -1.0656,  0.4911,\n",
      "         -0.2134, -0.0755,  1.2576, -0.4642,  0.6547, -0.5013, -1.3040,  0.4785,\n",
      "         -0.6658, -0.6290, -1.2277,  0.2386,  0.0071,  1.3634, -1.4969,  1.5981,\n",
      "         -1.3170,  0.1154,  0.0947, -0.0098,  0.2222,  1.8705,  0.5570,  1.3363,\n",
      "         -0.0200,  0.6287,  0.7449,  2.2021,  1.3439,  0.4340, -0.1555,  0.3650,\n",
      "          2.4762,  0.0602,  0.3540, -0.4365,  0.7448,  0.7881,  0.0611, -1.2449,\n",
      "         -1.2145,  0.9024,  0.7474,  0.2636,  1.4165,  0.6107, -0.6552, -0.8223,\n",
      "          0.8356, -0.0076,  0.4871, -1.3997,  1.9585, -1.0421,  0.3429,  1.1506,\n",
      "         -1.6929, -0.5112, -0.6647,  1.2353, -0.5246,  0.1558,  0.4594,  0.7230,\n",
      "          2.5120, -0.1763, -0.0087, -0.6539, -2.3145, -1.5196,  0.8503, -0.2056]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "        \"\"\"\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed\n",
    "\n",
    "class bind_network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(bind_network, self).__init__()\n",
    "        self.image_dim = args.image_dim    # CI \n",
    "        self.model_dim = args.model_dim    # C  \n",
    "        self.ffn_dim = self.model_dim * 4  # Ch \n",
    "\n",
    "        self.linear0 = nn.Linear(self.image_dim, self.model_dim, bias = False)  # w0 CI*C\n",
    "        \n",
    "        self.projection_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                RMSNorm(self.model_dim),                                        # RMSNorm\n",
    "                nn.Linear(self.model_dim, self.ffn_dim, bias = False),          # w1 C*Ch\n",
    "                nn.Linear(self.model_dim, self.ffn_dim, bias = False),          # w2 C*Ch\n",
    "                nn.Linear(self.ffn_dim, self.model_dim, bias = False),          # w3 Ch*C\n",
    "                nn.SiLU()                                                       # SiLU\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "    def forward(self, image_feature):        \n",
    "        F_I_0 = self.linear0(image_feature)  \n",
    "        F_I_i = F_I_0                        \n",
    "        for block in self.projection_blocks:\n",
    "            F_I_i = block[0](F_I_i)\n",
    "            F_I_i = F_I_i + block[3](block[2](F_I_i) * block[4](block[1](F_I_i)))\n",
    "\n",
    "        return F_I_i\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, image_dim, model_dim):\n",
    "        self.image_dim = image_dim\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "args = Args(image_dim = 64, model_dim = 512)\n",
    "\n",
    "bind_net = bind_network(args)\n",
    "\n",
    "image_feature = torch.randn(1, args.image_dim)\n",
    "\n",
    "output = bind_net(image_feature)\n",
    "\n",
    "print(f\"\"\"\n",
    "Image Shape: {image_feature.shape}\n",
    "Image Feature: {image_feature}\n",
    "Output Shape: {output.shape}\n",
    "Output: {output}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed0c75",
   "metadata": {},
   "source": [
    "## Q2\n",
    "The Transformer applies self-attention mechanism.\n",
    "Here is the corresponding code in the Fairseq-toolkit. Please write the line number(s)\n",
    "for the code(s) which do the scaling with the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ebbe5",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca74f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "self.scaling = self.head_dim ** 0.5  # Line 114\n",
    "q *= self.scaling                    # Line 602\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ff85e",
   "metadata": {},
   "source": [
    "## Q3\n",
    "RMSNorm is another regularizer which can replace the layer normalization.\n",
    "Read the paper and briefly illustrate the calculation process of RMSNorm and\n",
    "its advantages over Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb3362",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "The RNSNorm is calculated as below\n",
    "\n",
    "$$\\bar a_i = \\frac {a_i}{\\text {RMS}(\\vec a)}g_i, \\text {where RMS}(\\vec a) = \\sqrt{\\frac 1 n \\sum ^n_{i = 1} a_i^2}, i=1,2,\\dots, n.$$\n",
    "\n",
    "where $g_i$ is the gain parameter used to re-scale the standardized summed inputs, and is set to 1 at the beginning.\n",
    "\n",
    "One of the advantages of RMSNorm to LayerNorm is that RMSNorm simplifies computation, reducing running time by 6.9% to 40% on different models, while it performs comparable or better performance than LayerNorm on various tasks and neural architectures. Also, RMSNorm is invariant to the scaling of inputs and weights, which stabilizes the layer activations and the model gradients. It also has an implicit learning rate adaptation ability that avoids large-norm weight matrix and improves model convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b58548",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "LLM used: \n",
    "    \n",
    "**ChatGPT 3.5**\n",
    "\n",
    "请根据以下文字编写相应的代码：\n",
    "\n",
    "Bind Network.\n",
    "\n",
    "In Figure 3, we present the details of the bind network, which aims to align the embedding space between ImageBind and LLaMA. Specifically, we denote the CI -dimensional global image feature encoded by ImageBind as FI ∈ R^(1×CI). In the bind network, we first adopt a linear projection layer with a weight matrix w0 ∈ R^(CI×C) , formulated as F_I^0 = F_I w_0 ∈ R(1×C), where C denotes the feature dimension of LLaMA. Inspired by the Feed-Forward Network (FFN) in LLaMA, we then cascade three projection blocks with RMSNorm [43], SiLU activation functions [44], and residual connections [45]. For the (i + 1)-th block with F_I^i as input, we formulate the calculation of F_I^(i+1) as (the normalization is omitted for simplicity)\n",
    "\n",
    "F_I^(i+1) = F_I^i + (F_I^i w_2 · SiLU(F_I^i w_1))w_3, 0 ≤ i < 3   (1)\n",
    "\n",
    "where w1, w2 ∈ R^(C×Ch) and w3 ∈ R^(Ch×C) , with Ch denoting the hidden dimension. After the bind network, we obtain the transformed image feature, T_I ∈ R^(1×C) , which learns to align the embedding space from ImageBind to LLaMA\n",
    "\n",
    "**New Bing**\n",
    "\n",
    "Read the paper and summarize the advantages of RMSNorm when compared with LayerNorm\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
