{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779900ab-3219-4e5e-82ce-d0df9e70014f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4620d28-1d5b-4ccb-8ddf-190b503301b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/26 03:30:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local\", \"partitioning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c40c480-fdfb-41ef-96ef-db5e2f6c34b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_rdd = sc.textFile(\"./data/task1/departuredelays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a77dab1-65d8-4cc8-8a37-995d27ee512f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "header = data_rdd.first()\n",
    "data_rdd = data_rdd.filter(lambda line: line != header)\n",
    "\n",
    "def custom_partition(row):\n",
    "    origin = row.split(',')[3]\n",
    "    if origin == 'ATL':\n",
    "        return 0\n",
    "    else:\n",
    "        return hash(origin) % 3 + 1\n",
    "\n",
    "partitioned_rdd = data_rdd.map(lambda line: (custom_partition(line), line))\n",
    "\n",
    "for i in range(4):\n",
    "    partitioned_rdd.filter(lambda x: x[0] == i).values().saveAsTextFile(f\"./output/task1/partition_{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514341b5-8877-4b7b-8078-e1e6757f5cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 02:40:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\n",
    "        \"spark.jars\", \n",
    "        \"./kafka-clients-3.5.0.jar,./spark-sql-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "        ./spark-token-provider-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "        ./commons-pool2-2.12.0.jar\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"3\") \\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\",\"/root/anaconda3/bin/python\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\") \\\n",
    "    .appName(\"HW03\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248f449d-7400-489e-8c28-e4a17ee78140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"./data/task2/activity-data/\") \\\n",
    "    .schema(\"Arrival_Time long, Creation_Time long, Device string, Index long, Model string, User string, gt string, x double, y double, z double\") \\\n",
    "    .load()\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fed0ac-5f37-4201-ae5b-fa326b39f4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 00:54:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/05/08 00:54:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 5800 milliseconds\n"
     ]
    }
   ],
   "source": [
    "window_query = streaming_df.withColumn(\"Creation_Time_seconds\", col(\"Creation_Time\") / 1000000000) \\\n",
    "    .withColumn(\"Creation_Time_timestamp\", from_unixtime(col(\"Creation_Time_seconds\"))) \\\n",
    "    .groupBy(\"User\", window(\"Creation_Time_timestamp\", \"6 minute\", \"3 minute\")).count() \\\n",
    "    .writeStream.queryName(\"activity_query\").format(\"memory\").outputMode(\"update\").trigger(processingTime=\"2 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c760b165-44a6-4bca-aac2-bbe89aba10d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------+-----+\n",
      "|User|window                                    |count|\n",
      "+----+------------------------------------------+-----+\n",
      "|g   |{2015-02-23 10:45:00, 2015-02-23 10:51:00}|62201|\n",
      "|g   |{2015-02-23 10:15:00, 2015-02-23 10:21:00}|18846|\n",
      "|g   |{2015-02-23 10:21:00, 2015-02-23 10:27:00}|57165|\n",
      "+----+------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_df = spark.sql(\"select * from activity_query\")\n",
    "window_df.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78771402-14fb-42e6-94c5-2d93e42223e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_updated = None\n",
    "\n",
    "def write_to_memory_and_parquet(df, epoch_id):\n",
    "    global df_updated\n",
    "    if df_updated is None:\n",
    "        df_updated = df.toDF(\"User\", \"window\", \"count\")\n",
    "    else:\n",
    "        df_updated = df_updated.union(df.toDF(\"User\", \"window\", \"count\"))\n",
    "    df.persist() \n",
    "    df.write.format(\"parquet\").mode(\"append\").save(\"./output/task2/parquet\")\n",
    "\n",
    "window_query_updated = streaming_df.withColumn(\"Creation_Time_seconds\", col(\"Creation_Time\") / 1000000000) \\\n",
    "    .withColumn(\"Creation_Time_timestamp\", from_unixtime(col(\"Creation_Time_seconds\"))) \\\n",
    "    .groupBy(\"User\", window(\"Creation_Time_timestamp\", \"6 minute\", \"3 minute\")).count() \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"activity_query\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(write_to_memory_and_parquet) \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"./ckpt_parquet/\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c625ff39-5606-4baf-a0d2-61e30fdbc16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------+-----+\n",
      "|User|window                                    |count|\n",
      "+----+------------------------------------------+-----+\n",
      "|g   |{2015-02-23 10:45:00, 2015-02-23 10:51:00}|62201|\n",
      "|g   |{2015-02-23 10:15:00, 2015-02-23 10:21:00}|18846|\n",
      "|g   |{2015-02-23 10:21:00, 2015-02-23 10:27:00}|57165|\n",
      "|g   |{2015-02-23 10:54:00, 2015-02-23 11:00:00}|92614|\n",
      "|g   |{2015-02-23 10:24:00, 2015-02-23 10:30:00}|64541|\n",
      "|g   |{2015-02-23 11:03:00, 2015-02-23 11:09:00}|67425|\n",
      "|g   |{2015-02-23 10:36:00, 2015-02-23 10:42:00}|59742|\n",
      "|g   |{2015-02-23 11:09:00, 2015-02-23 11:15:00}|55430|\n",
      "|g   |{2015-02-23 11:12:00, 2015-02-23 11:18:00}|53499|\n",
      "|g   |{2015-02-23 10:42:00, 2015-02-23 10:48:00}|55498|\n",
      "|g   |{2015-02-23 11:18:00, 2015-02-23 11:24:00}|57777|\n",
      "|g   |{2015-02-23 11:21:00, 2015-02-23 11:27:00}|55715|\n",
      "|c   |{2015-02-23 12:42:00, 2015-02-23 12:48:00}|65718|\n",
      "|c   |{2015-02-23 12:09:00, 2015-02-23 12:15:00}|14805|\n",
      "|c   |{2015-02-23 12:18:00, 2015-02-23 12:24:00}|61558|\n",
      "|c   |{2015-02-23 12:30:00, 2015-02-23 12:36:00}|62532|\n",
      "|c   |{2015-02-23 13:03:00, 2015-02-23 13:09:00}|64803|\n",
      "|c   |{2015-02-23 12:33:00, 2015-02-23 12:39:00}|62564|\n",
      "|c   |{2015-02-23 13:06:00, 2015-02-23 13:12:00}|55610|\n",
      "|a   |{2015-02-23 13:30:00, 2015-02-23 13:36:00}|52282|\n",
      "+----+------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_updated.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6595da4b-1f7c-412c-b94c-230863d56865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q3_df = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"q3\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "q3_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2135eaba-6771-46f1-b242-6ea3763febe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 03:09:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"q3\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert value column from binary to string\n",
    "streaming_df = streaming_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse CSV content and select relevant columns\n",
    "parsed_df = streaming_df.selectExpr(\"split(value, ',') as columns\") \\\n",
    "    .selectExpr(\"columns[7] as action\", \"columns[9] as gender\")\n",
    "\n",
    "# Filter action value equal to 2\n",
    "filtered_df = parsed_df.filter(\"action = '2' and (gender = '0' or gender = '1')\")\n",
    "\n",
    "# Define watermark delay\n",
    "watermark_delay = \"10 seconds\"\n",
    "\n",
    "# Define windowed aggregation\n",
    "windowed_df = filtered_df \\\n",
    "    .withColumn(\"timestamp\", expr(\"current_timestamp()\")) \\\n",
    "    .withWatermark(\"timestamp\", watermark_delay) \\\n",
    "    .groupBy(window(\"timestamp\", \"5 seconds\"), \"gender\") \\\n",
    "    .count()\n",
    "\n",
    "# Write the results into memory with complete mode\n",
    "query = windowed_df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"transaction_count\") \\\n",
    "    .option(\"checkpointLocation\", \"./q3_checkpoint\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6607868-0bae-4cfa-a8d7-eef0297478f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system(\"kafka-topics.sh --bootstrap-server localhost:9092 --topic q3 --delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e01758f-457e-4fe1-bf8d-e9c940856b07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+-----+\n",
      "|window                                    |gender|count|\n",
      "+------------------------------------------+------+-----+\n",
      "|{2024-05-05 03:10:00, 2024-05-05 03:10:05}|0     |161  |\n",
      "|{2024-05-05 03:14:30, 2024-05-05 03:14:35}|0     |119  |\n",
      "|{2024-05-05 03:18:00, 2024-05-05 03:18:05}|0     |133  |\n",
      "|{2024-05-05 03:17:45, 2024-05-05 03:17:50}|1     |150  |\n",
      "|{2024-05-05 03:12:40, 2024-05-05 03:12:45}|0     |104  |\n",
      "|{2024-05-05 03:12:30, 2024-05-05 03:12:35}|1     |117  |\n",
      "|{2024-05-05 03:13:55, 2024-05-05 03:14:00}|1     |107  |\n",
      "|{2024-05-05 03:11:20, 2024-05-05 03:11:25}|0     |150  |\n",
      "|{2024-05-05 03:10:25, 2024-05-05 03:10:30}|0     |165  |\n",
      "|{2024-05-05 03:11:35, 2024-05-05 03:11:40}|0     |153  |\n",
      "|{2024-05-05 03:11:50, 2024-05-05 03:11:55}|1     |137  |\n",
      "|{2024-05-05 03:13:00, 2024-05-05 03:13:05}|0     |137  |\n",
      "|{2024-05-05 03:09:15, 2024-05-05 03:09:20}|0     |117  |\n",
      "|{2024-05-05 03:14:00, 2024-05-05 03:14:05}|1     |149  |\n",
      "|{2024-05-05 03:17:50, 2024-05-05 03:17:55}|0     |126  |\n",
      "|{2024-05-05 03:13:40, 2024-05-05 03:13:45}|0     |149  |\n",
      "|{2024-05-05 03:13:25, 2024-05-05 03:13:30}|1     |129  |\n",
      "|{2024-05-05 03:12:45, 2024-05-05 03:12:50}|0     |130  |\n",
      "|{2024-05-05 03:09:30, 2024-05-05 03:09:35}|0     |149  |\n",
      "|{2024-05-05 03:09:40, 2024-05-05 03:09:45}|1     |153  |\n",
      "+------------------------------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transaction_count\").show(20, truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
